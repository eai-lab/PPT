model_name: 'patchtst'


patch_len: 5
stride: 5
revin: False
pe: sincos
learn_pe: False
padding_patch: null
n_layers: 3
d_model: 64
n_heads: 4
d_k: null  # d_model // n_heads if d_k is None
d_v: null
d_ff: 256
norm: 'BatchNorm'
attn_dropout: 0.0
dropout: 0.0
act: 'gelu'
key_padding_mask: 'auto'
padding_var: null 
attn_mask: null 
res_attention: True
pre_norm: False
store_attn: False


### Fine-Tuning ###
head_type: lstm # lstm, last, cls